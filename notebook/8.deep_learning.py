# -*- coding: utf-8 -*-
"""Deep Learning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qiNkee7bBh1Uhfh6JHZBstU6bf7o6UwA

#Deep Learning
"""

!pip install -q transformers accelerate peft evaluate

"""##Demographic Dataset"""

# ===============================================================
# DEMOGRAPHIC DEEP LEARNING ‚Äî IMPORT & KONFIGURASI
# ===============================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import (
    classification_report, confusion_matrix,
    accuracy_score, f1_score
)
from sklearn.utils.class_weight import compute_class_weight

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

import torch
from torch.utils.data import Dataset
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    TrainingArguments, Trainer
)
from peft import LoraConfig, get_peft_model
import evaluate

# üîÅ Seed supaya hasil stabil
SEED = 42
np.random.seed(SEED)
tf.random.set_seed(SEED)
torch.manual_seed(SEED)

# ‚öôÔ∏è Konfigurasi LITE (lebih hemat RAM)
MAX_WORDS = 8000      # sebelumnya 20000
MAX_LEN   = 120       # sebelumnya 150
EMBED_DIM = 64        # sebelumnya 256

print("Konfigurasi LITE:")
print("MAX_WORDS =", MAX_WORDS)
print("MAX_LEN   =", MAX_LEN)
print("EMBED_DIM =", EMBED_DIM)

# =========================================================
# LOAD DATASET DEMOGRAPHIC & ENCODE LABEL
# =========================================================

df_demo = pd.read_csv("/content/demographics_with_qi_labels.csv")

assert "clean_text" in df_demo.columns
assert "label" in df_demo.columns

df_demo["clean_text"] = df_demo["clean_text"].fillna("").astype(str)

# Encode HIGH_RISK / LOW_RISK ‚Üí 0 / 1
le = LabelEncoder()
df_demo["label_id"] = le.fit_transform(df_demo["label"])

print("Kelas:", list(le.classes_))
print(df_demo["label_id"].value_counts())

texts_demo  = df_demo["clean_text"].tolist()
labels_demo = df_demo["label_id"].tolist()

X_train_demo, X_test_demo, y_train_demo, y_test_demo = train_test_split(
    texts_demo,
    labels_demo,
    test_size=0.2,
    stratify=labels_demo,
    random_state=SEED
)

print("Train =", len(X_train_demo), " | Test =", len(X_test_demo))

# =========================================================
# TOKENIZER & PADDING ‚Äî DEMOGRAPHIC
# =========================================================

tokenizer_demo = Tokenizer(num_words=MAX_WORDS, lower=True, oov_token="<OOV>")
tokenizer_demo.fit_on_texts(X_train_demo)

X_train_demo_seq = pad_sequences(
    tokenizer_demo.texts_to_sequences(X_train_demo),
    maxlen=MAX_LEN,
    padding="post"
)
X_test_demo_seq = pad_sequences(
    tokenizer_demo.texts_to_sequences(X_test_demo),
    maxlen=MAX_LEN,
    padding="post"
)

y_train_demo_arr = np.array(y_train_demo)
y_test_demo_arr  = np.array(y_test_demo)

X_train_demo_seq.shape, X_test_demo_seq.shape

# =========================================================
# CLASS WEIGHT ‚Äî DEMOGRAPHIC
# =========================================================

classes_demo = np.unique(y_train_demo_arr)
cw_values_demo = compute_class_weight(
    class_weight="balanced",
    classes=classes_demo,
    y=y_train_demo_arr
)
class_weights_demo = dict(zip(classes_demo, cw_values_demo))

print("Class Weights (Demographic):", class_weights_demo)

"""###RNN"""

# =========================================================
# Model RNN sederhana
# Tujuan:
# - Menjadi baseline model berbasis sekuens untuk klasifikasi risiko.
# =========================================================

def build_rnn_demo():
    model = models.Sequential([
        layers.Input(shape=(MAX_LEN,)),
        layers.Embedding(MAX_WORDS, EMBED_DIM),
        layers.Bidirectional(layers.SimpleRNN(32, return_sequences=True)),  # 128 ‚Üí 32
        layers.Dropout(0.3),
        layers.Bidirectional(layers.SimpleRNN(16)),                         # 64 ‚Üí 16
        layers.Dense(32, activation="relu"),
        layers.Dropout(0.3),
        layers.Dense(1, activation="sigmoid")
    ])
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=8e-4),
        loss="binary_crossentropy",
        metrics=["accuracy"]
    )
    return model

rnn_demo = build_rnn_demo()
rnn_demo.summary()

print("Training RNN (Demographic)...")
hist_rnn_demo = rnn_demo.fit(
    X_train_demo_seq,
    y_train_demo_arr,
    validation_split=0.1,
    epochs=8,               # 12 ‚Üí 8 (lebih cepat)
    batch_size=32,
    class_weight=class_weights_demo,
    verbose=1
)

pred_rnn_demo_prob = rnn_demo.predict(X_test_demo_seq)
pred_rnn_demo = (pred_rnn_demo_prob >= 0.5).astype(int).flatten()

print("Distribusi prediksi RNN:", np.unique(pred_rnn_demo, return_counts=True))
print(classification_report(
    y_test_demo_arr,
    pred_rnn_demo,
    target_names=le.classes_,
    digits=4
))

"""###LSTM"""

# =========================================================
# Model LSTM
# Tujuan:
# - Menggunakan memori jangka panjang untuk data sekuensial.
# =========================================================

def build_lstm_demo():
    model = models.Sequential([
        layers.Input(shape=(MAX_LEN,)),
        layers.Embedding(MAX_WORDS, EMBED_DIM),
        layers.Bidirectional(layers.LSTM(32, return_sequences=True)),  # 128 ‚Üí 32
        layers.Dropout(0.3),
        layers.Bidirectional(layers.LSTM(16)),                         # 64 ‚Üí 16
        layers.Dense(32, activation="relu"),
        layers.Dropout(0.3),
        layers.Dense(1, activation="sigmoid")
    ])
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=8e-4),
        loss="binary_crossentropy",
        metrics=["accuracy"]
    )
    return model

lstm_demo = build_lstm_demo()
lstm_demo.summary()

print("Training LSTM (Demographic)...")
hist_lstm_demo = lstm_demo.fit(
    X_train_demo_seq,
    y_train_demo_arr,
    validation_split=0.1,
    epochs=10,              # 15 ‚Üí 10
    batch_size=32,
    class_weight=class_weights_demo,
    verbose=1
)

pred_lstm_demo_prob = lstm_demo.predict(X_test_demo_seq)
pred_lstm_demo = (pred_lstm_demo_prob >= 0.5).astype(int).flatten()

print("Distribusi prediksi LSTM:", np.unique(pred_lstm_demo, return_counts=True))
print(classification_report(
    y_test_demo_arr,
    pred_lstm_demo,
    target_names=le.classes_,
    digits=4
))

"""###Fine Tunning"""

# =========================================================
# Fine-Tuned LSTM
# Tujuan:
# - Menggunakan embedding berdimensi lebih tinggi.
# - Meningkatkan kapasitas model untuk menangkap konteks risiko QI.
# =========================================================

def to_pad_demo(texts, mlen):
    seq = tokenizer_demo.texts_to_sequences(texts)
    return pad_sequences(seq, maxlen=mlen, padding="post")

# Search space diperkecil: tetap 2 config, tapi units kecil
search_space_demo = [
    {"max_len": 120, "u1": 32, "u2": 16},
    {"max_len": 150, "u1": 32, "u2": 16},
]

X_train_ft_demo, X_val_ft_demo, y_train_ft_demo, y_val_ft_demo = train_test_split(
    X_train_demo, y_train_demo,
    test_size=0.1,
    stratify=y_train_demo,
    random_state=SEED
)

best_cfg_demo = None
best_acc_demo = 0.0

for cfg in search_space_demo:
    print("\nTesting config (Demographic):", cfg)
    Xtr = to_pad_demo(X_train_ft_demo, cfg["max_len"])
    Xv  = to_pad_demo(X_val_ft_demo,  cfg["max_len"])

    model = models.Sequential([
        layers.Input(shape=(cfg["max_len"],)),
        layers.Embedding(MAX_WORDS, EMBED_DIM),
        layers.Bidirectional(layers.LSTM(cfg["u1"], return_sequences=True)),
        layers.Dropout(0.3),
        layers.Bidirectional(layers.LSTM(cfg["u2"])),
        layers.Dropout(0.3),
        layers.Dense(64, activation="relu"),
        layers.Dropout(0.3),
        layers.Dense(1, activation="sigmoid")
    ])
    model.compile(
        optimizer=tf.keras.optimizers.Adam(1e-3),
        loss="binary_crossentropy",
        metrics=["accuracy"]
    )

    hist = model.fit(
        Xtr, np.array(y_train_ft_demo),
        validation_data=(Xv, np.array(y_val_ft_demo)),
        epochs=3,          # 8 ‚Üí 3 (cuma untuk cari config)
        batch_size=32,
        class_weight=class_weights_demo,
        verbose=0
    )

    acc = max(hist.history["val_accuracy"])
    print("Best val acc:", acc)

    if acc > best_acc_demo:
        best_acc_demo = acc
        best_cfg_demo = cfg

print("\nConfig terbaik (Demographic):", best_cfg_demo)
print("Val Accuracy terbaik:", best_acc_demo)

# ===== Train FT-LSTM final =====
X_train_final_demo = to_pad_demo(X_train_demo, best_cfg_demo["max_len"])
X_test_final_demo  = to_pad_demo(X_test_demo,  best_cfg_demo["max_len"])

ft_lstm_demo = models.Sequential([
    layers.Input(shape=(best_cfg_demo["max_len"],)),
    layers.Embedding(MAX_WORDS, EMBED_DIM),
    layers.Bidirectional(layers.LSTM(best_cfg_demo["u1"], return_sequences=True)),
    layers.Dropout(0.3),
    layers.Bidirectional(layers.LSTM(best_cfg_demo["u2"])),
    layers.Dropout(0.3),
    layers.Dense(64, activation="relu"),
    layers.Dropout(0.3),
    layers.Dense(1, activation="sigmoid")
])

ft_lstm_demo.compile(
    optimizer=tf.keras.optimizers.Adam(1e-3),
    loss="binary_crossentropy",
    metrics=["accuracy"]
)

early_stop_demo = tf.keras.callbacks.EarlyStopping(
    monitor="val_loss",
    patience=3,
    restore_best_weights=True
)

print("Training FT-LSTM final (Demographic)...")
hist_ft_demo = ft_lstm_demo.fit(
    X_train_final_demo,
    y_train_demo_arr,
    validation_split=0.1,
    epochs=8,            # 20 ‚Üí 8
    batch_size=32,
    class_weight=class_weights_demo,
    callbacks=[early_stop_demo],
    verbose=1
)

pred_ft_demo_prob = ft_lstm_demo.predict(X_test_final_demo)
pred_ft_demo = (pred_ft_demo_prob >= 0.5).astype(int).flatten()

print("Distribusi prediksi FT-LSTM:", np.unique(pred_ft_demo, return_counts=True))
print(classification_report(
    y_test_demo_arr,
    pred_ft_demo,
    target_names=le.classes_,
    digits=4
))

"""###LoRa"""

import torch
import numpy as np
import pandas as pd
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from peft import LoraConfig, get_peft_model
from sklearn.metrics import classification_report, accuracy_score, f1_score
from sklearn.model_selection import train_test_split

# Cek GPU
print("CUDA available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("GPU:", torch.cuda.get_device_name(0))
device = "cuda" if torch.cuda.is_available() else "cpu"

class TextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=128):
        self.texts = texts
        self.labels = labels
        self.tok = tokenizer
        self.max_len = max_len

    def __len__(self): return len(self.texts)

    def __getitem__(self, idx):
        enc = self.tok(
            self.texts[idx],
            truncation=True,
            padding="max_length",
            max_length=self.max_len,
            return_tensors="pt"
        )
        item = {k: v.squeeze(0) for k, v in enc.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

MODEL_NAME = "cahya/bert-base-indonesian-1.5G"

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

base_model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=2
).to(device)

lora_cfg = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.1,
    target_modules=["query","key","value"],
    task_type="SEQ_CLS"
)

model = get_peft_model(base_model, lora_cfg)
model = model.to(device)

def train_lora(X_train, X_test, y_train, y_test, tokenizer, model, epochs=2):
    train_ds = TextDataset(X_train, y_train, tokenizer)
    test_ds  = TextDataset(X_test,  y_test,  tokenizer)

    # Batch kecil = aman di GPU RAM 15GB
    train_dl = DataLoader(train_ds, batch_size=2, shuffle=True)
    test_dl  = DataLoader(test_ds,  batch_size=2)

    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
    loss_fn = nn.CrossEntropyLoss()

    for epoch in range(epochs):
        model.train()
        total_loss = 0

        for batch in train_dl:
            # Pindah ke GPU
            batch = {k: v.to(device) for k, v in batch.items()}

            outputs = model(**batch)
            loss = outputs.loss

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        print(f"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(train_dl):.4f}")

    # Evaluasi
    model.eval()
    preds, trues = [], []

    with torch.no_grad():
        for batch in test_dl:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)
            logits = outputs.logits
            p = torch.argmax(logits, dim=1).cpu().numpy()
            t = batch['labels'].cpu().numpy()
            preds.extend(p)
            trues.extend(t)

    print("\nClassification Report:")
    print(classification_report(trues, preds, digits=4))
    print("Accuracy:", accuracy_score(trues, preds))
    print("F1:", f1_score(trues, preds, average="weighted"))

    return preds, trues

df_demo = pd.read_csv("/content/demographics_with_qi_labels.csv")

df_demo["clean_text"] = df_demo["clean_text"].astype(str)
df_demo["label_id"] = df_demo["label"].astype("category").cat.codes

X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(
    df_demo["clean_text"].tolist(),
    df_demo["label_id"].tolist(),
    test_size=0.2,
    stratify=df_demo["label_id"].tolist(),
    random_state=42
)

print("=== TRAINING DEMOGRAPHIC ===")
preds_demo, true_demo = train_lora(
    X_train_d, X_test_d, y_train_d, y_test_d,
    tokenizer, model, epochs=2
)

"""#Deep Learning

##Clinical Dataset
"""

# ===============================================================
# CLINICAL DEEP LEARNING ‚Äî IMPORT & KONFIGURASI
# ===============================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import (
    classification_report, confusion_matrix,
    accuracy_score, f1_score
)
from sklearn.utils.class_weight import compute_class_weight

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

import torch
from torch.utils.data import Dataset
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    TrainingArguments, Trainer
)
from peft import LoraConfig, get_peft_model
import evaluate

SEED = 42
np.random.seed(SEED)
tf.random.set_seed(SEED)
torch.manual_seed(SEED)

# Pakai konfigurasi LITE yang sama
MAX_WORDS = 8000
MAX_LEN   = 120
EMBED_DIM = 64

print("Konfigurasi LITE (Clinical):")
print("MAX_WORDS =", MAX_WORDS)
print("MAX_LEN   =", MAX_LEN)
print("EMBED_DIM =", EMBED_DIM)

# Pakai konfigurasi LITE yang sama
MAX_WORDS = 8000
MAX_LEN   = 120
EMBED_DIM = 64

print("Konfigurasi LITE (Clinical):")
print("MAX_WORDS =", MAX_WORDS)
print("MAX_LEN   =", MAX_LEN)
print("EMBED_DIM =", EMBED_DIM)

# =========================================================
# LOAD DATASET CLINICAL & ENCODE LABEL
# =========================================================

df_clin = pd.read_csv("/content/ClinicalData_with_qi_labels.csv")

assert "clean_text" in df_clin.columns
assert "label" in df_clin.columns

df_clin["clean_text"] = df_clin["clean_text"].fillna("").astype(str)

le_clin = LabelEncoder()
df_clin["label_id"] = le_clin.fit_transform(df_clin["label"])

print("Kelas (Clinical):", list(le_clin.classes_))
print(df_clin["label_id"].value_counts())

texts_clin  = df_clin["clean_text"].tolist()
labels_clin = df_clin["label_id"].tolist()

X_train_clin, X_test_clin, y_train_clin, y_test_clin = train_test_split(
    texts_clin,
    labels_clin,
    test_size=0.2,
    stratify=labels_clin,
    random_state=SEED
)

print("Train =", len(X_train_clin), " | Test =", len(X_test_clin))

# =========================================================
# TOKENIZER & PADDING ‚Äî CLINICAL
# =========================================================

tokenizer_clin = Tokenizer(num_words=MAX_WORDS, lower=True, oov_token="<OOV>")
tokenizer_clin.fit_on_texts(X_train_clin)

X_train_clin_seq = pad_sequences(
    tokenizer_clin.texts_to_sequences(X_train_clin),
    maxlen=MAX_LEN,
    padding="post"
)
X_test_clin_seq = pad_sequences(
    tokenizer_clin.texts_to_sequences(X_test_clin),
    maxlen=MAX_LEN,
    padding="post"
)

y_train_clin_arr = np.array(y_train_clin)
y_test_clin_arr  = np.array(y_test_clin)

X_train_clin_seq.shape, X_test_clin_seq.shape

# =========================================================
# CLASS WEIGHT ‚Äî CLINICAL
# =========================================================

classes_clin = np.unique(y_train_clin_arr)
cw_values_clin = compute_class_weight(
    class_weight="balanced",
    classes=classes_clin,
    y=y_train_clin_arr
)
class_weights_clin = dict(zip(classes_clin, cw_values_clin))

print("Class Weights (Clinical):", class_weights_clin)

"""###RNN"""

# =========================================================
# RNN (BIDIRECTIONAL) ‚Äî CLINICAL (LITE)
# =========================================================

def build_rnn_clin():
    model = models.Sequential([
        layers.Input(shape=(MAX_LEN,)),
        layers.Embedding(MAX_WORDS, EMBED_DIM),
        layers.Bidirectional(layers.SimpleRNN(32, return_sequences=True)),
        layers.Dropout(0.3),
        layers.Bidirectional(layers.SimpleRNN(16)),
        layers.Dense(32, activation="relu"),
        layers.Dropout(0.3),
        layers.Dense(1, activation="sigmoid")
    ])
    model.compile(
        optimizer=tf.keras.optimizers.Adam(8e-4),
        loss="binary_crossentropy",
        metrics=["accuracy"]
    )
    return model

rnn_clin = build_rnn_clin()
rnn_clin.summary()

print("Training RNN (Clinical)...")
hist_rnn_clin = rnn_clin.fit(
    X_train_clin_seq,
    y_train_clin_arr,
    validation_split=0.1,
    epochs=8,
    batch_size=32,
    class_weight=class_weights_clin,
    verbose=1
)

pred_rnn_clin_prob = rnn_clin.predict(X_test_clin_seq)
pred_rnn_clin = (pred_rnn_clin_prob >= 0.5).astype(int).flatten()

print("Distribusi prediksi RNN Clinical:", np.unique(pred_rnn_clin, return_counts=True))
print(classification_report(
    y_test_clin_arr,
    pred_rnn_clin,
    target_names=le_clin.classes_,
    digits=4
))

"""###LSTM"""

# =========================================================
# LSTM (BIDIRECTIONAL) ‚Äî CLINICAL (LITE)
# =========================================================

def build_lstm_clin():
    model = models.Sequential([
        layers.Input(shape=(MAX_LEN,)),
        layers.Embedding(MAX_WORDS, EMBED_DIM),
        layers.Bidirectional(layers.LSTM(32, return_sequences=True)),
        layers.Dropout(0.3),
        layers.Bidirectional(layers.LSTM(16)),
        layers.Dense(32, activation="relu"),
        layers.Dropout(0.3),
        layers.Dense(1, activation="sigmoid")
    ])
    model.compile(
        optimizer=tf.keras.optimizers.Adam(8e-4),
        loss="binary_crossentropy",
        metrics=["accuracy"]
    )
    return model

lstm_clin = build_lstm_clin()
lstm_clin.summary()

print("Training LSTM (Clinical)...")
hist_lstm_clin = lstm_clin.fit(
    X_train_clin_seq,
    y_train_clin_arr,
    validation_split=0.1,
    epochs=10,
    batch_size=32,
    class_weight=class_weights_clin,
    verbose=1
)

pred_lstm_clin_prob = lstm_clin.predict(X_test_clin_seq)
pred_lstm_clin = (pred_lstm_clin_prob >= 0.5).astype(int).flatten()

print("Distribusi prediksi LSTM Clinical:", np.unique(pred_lstm_clin, return_counts=True))
print(classification_report(
    y_test_clin_arr,
    pred_lstm_clin,
    target_names=le_clin.classes_,
    digits=4
))

"""###Fine Tunning"""

# =========================================================
# FT-LSTM ‚Äî CLINICAL (LITE)
# =========================================================

def to_pad_clin(texts, mlen):
    seq = tokenizer_clin.texts_to_sequences(texts)
    return pad_sequences(seq, maxlen=mlen, padding="post")

search_space_clin = [
    {"max_len": 120, "u1": 32, "u2": 16},
    {"max_len": 150, "u1": 32, "u2": 16},
]

X_train_ft_clin, X_val_ft_clin, y_train_ft_clin, y_val_ft_clin = train_test_split(
    X_train_clin, y_train_clin,
    test_size=0.1,
    stratify=y_train_clin,
    random_state=SEED
)

best_cfg_clin = None
best_acc_clin = 0.0

for cfg in search_space_clin:
    print("\nTesting config (Clinical):", cfg)
    Xtr = to_pad_clin(X_train_ft_clin, cfg["max_len"])
    Xv  = to_pad_clin(X_val_ft_clin,  cfg["max_len"])

    model = models.Sequential([
        layers.Input(shape=(cfg["max_len"],)),
        layers.Embedding(MAX_WORDS, EMBED_DIM),
        layers.Bidirectional(layers.LSTM(cfg["u1"], return_sequences=True)),
        layers.Dropout(0.3),
        layers.Bidirectional(layers.LSTM(cfg["u2"])),
        layers.Dropout(0.3),
        layers.Dense(64, activation="relu"),
        layers.Dropout(0.3),
        layers.Dense(1, activation="sigmoid")
    ])
    model.compile(
        optimizer=tf.keras.optimizers.Adam(1e-3),
        loss="binary_crossentropy",
        metrics=["accuracy"]
    )

    hist = model.fit(
        Xtr, np.array(y_train_ft_clin),
        validation_data=(Xv, np.array(y_val_ft_clin)),
        epochs=3,
        batch_size=32,
        class_weight=class_weights_clin,
        verbose=0
    )

    acc = max(hist.history["val_accuracy"])
    print("Best val acc:", acc)

    if acc > best_acc_clin:
        best_acc_clin = acc
        best_cfg_clin = cfg

print("\nConfig terbaik (Clinical):", best_cfg_clin)
print("Val Accuracy terbaik:", best_acc_clin)

# Train final FT-LSTM
X_train_final_clin = to_pad_clin(X_train_clin, best_cfg_clin["max_len"])
X_test_final_clin  = to_pad_clin(X_test_clin,  best_cfg_clin["max_len"])

ft_lstm_clin = models.Sequential([
    layers.Input(shape=(best_cfg_clin["max_len"],)),
    layers.Embedding(MAX_WORDS, EMBED_DIM),
    layers.Bidirectional(layers.LSTM(best_cfg_clin["u1"], return_sequences=True)),
    layers.Dropout(0.3),
    layers.Bidirectional(layers.LSTM(best_cfg_clin["u2"])),
    layers.Dropout(0.3),
    layers.Dense(64, activation="relu"),
    layers.Dropout(0.3),
    layers.Dense(1, activation="sigmoid")
])

ft_lstm_clin.compile(
    optimizer=tf.keras.optimizers.Adam(1e-3),
    loss="binary_crossentropy",
    metrics=["accuracy"]
)

early_stop_clin = tf.keras.callbacks.EarlyStopping(
    monitor="val_loss",
    patience=3,
    restore_best_weights=True
)

print("Training FT-LSTM final (Clinical)...")
hist_ft_clin = ft_lstm_clin.fit(
    X_train_final_clin,
    y_train_clin_arr,
    validation_split=0.1,
    epochs=8,
    batch_size=32,
    class_weight=class_weights_clin,
    callbacks=[early_stop_clin],
    verbose=1
)

pred_ft_clin_prob = ft_lstm_clin.predict(X_test_final_clin)
pred_ft_clin = (pred_ft_clin_prob >= 0.5).astype(int).flatten()

print("Distribusi prediksi FT-LSTM Clinical:", np.unique(pred_ft_clin, return_counts=True))
print(classification_report(
    y_test_clin_arr,
    pred_ft_clin,
    target_names=le_clin.classes_,
    digits=4
))

"""###LoRa"""

import torch
import numpy as np
import pandas as pd
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from peft import LoraConfig, get_peft_model
from sklearn.metrics import classification_report, accuracy_score, f1_score
from sklearn.model_selection import train_test_split

# Cek GPU
print("CUDA available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("GPU:", torch.cuda.get_device_name(0))
device = "cuda" if torch.cuda.is_available() else "cpu"

class TextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=128):
        self.texts = texts
        self.labels = labels
        self.tok = tokenizer
        self.max_len = max_len

    def __len__(self): return len(self.texts)

    def __getitem__(self, idx):
        enc = self.tok(
            self.texts[idx],
            truncation=True,
            padding="max_length",
            max_length=self.max_len,
            return_tensors="pt"
        )
        item = {k: v.squeeze(0) for k, v in enc.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

MODEL_NAME = "cahya/bert-base-indonesian-1.5G"

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

base_model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=2
).to(device)

lora_cfg = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.1,
    target_modules=["query","key","value"],
    task_type="SEQ_CLS"
)

model = get_peft_model(base_model, lora_cfg)
model = model.to(device)

def train_lora(X_train, X_test, y_train, y_test, tokenizer, model, epochs=2):
    train_ds = TextDataset(X_train, y_train, tokenizer)
    test_ds  = TextDataset(X_test,  y_test,  tokenizer)

    # Batch kecil = aman di GPU RAM 15GB
    train_dl = DataLoader(train_ds, batch_size=2, shuffle=True)
    test_dl  = DataLoader(test_ds,  batch_size=2)

    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
    loss_fn = nn.CrossEntropyLoss()

    for epoch in range(epochs):
        model.train()
        total_loss = 0

        for batch in train_dl:
            # Pindah ke GPU
            batch = {k: v.to(device) for k, v in batch.items()}

            outputs = model(**batch)
            loss = outputs.loss

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        print(f"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(train_dl):.4f}")

    # Evaluasi
    model.eval()
    preds, trues = [], []

    with torch.no_grad():
        for batch in test_dl:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)
            logits = outputs.logits
            p = torch.argmax(logits, dim=1).cpu().numpy()
            t = batch['labels'].cpu().numpy()
            preds.extend(p)
            trues.extend(t)

    print("\nClassification Report:")
    print(classification_report(trues, preds, digits=4))
    print("Accuracy:", accuracy_score(trues, preds))
    print("F1:", f1_score(trues, preds, average="weighted"))

    return preds, trues

df_demo = pd.read_csv("/content/ClinicalData_with_qi_labels.csv")

df_demo["clean_text"] = df_demo["clean_text"].astype(str)
df_demo["label_id"] = df_demo["label"].astype("category").cat.codes

X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(
    df_demo["clean_text"].tolist(),
    df_demo["label_id"].tolist(),
    test_size=0.2,
    stratify=df_demo["label_id"].tolist(),
    random_state=42
)

print("=== TRAINING CLINICAL ===")
preds_demo, true_demo = train_lora(
    X_train_d, X_test_d, y_train_d, y_test_d,
    tokenizer, model, epochs=2
)